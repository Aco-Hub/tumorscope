{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca7d2cf-d080-40c5-8328-6a8c3e380548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "import skimage as ski\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import normalize, resize, rotate\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from cellpose import models\n",
    "from skimage.measure import regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c95e1-8de1-479e-a960-5857d12ea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tifffile import imread\n",
    "\n",
    "root_path = Path(\"../../pvc/scratch/interaction_cells/datasets/\")\n",
    "print(\"Loading image...\")\n",
    "image = imread(root_path / \"series003_cCAR_tumor.tif\")\n",
    "print(\"Loaded image\")\n",
    "image = image[:, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e6c97b-2de0-4680-bfac-367aad216530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [00:01, 91.30it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Resizing...\")\n",
    "example_image =  ski.transform.resize(image, (image.shape[0], image.shape[1] // 2, image.shape[2] // 2), anti_aliasing=True)\n",
    "\n",
    "\n",
    "# Load corresponding masks\n",
    "\n",
    "\n",
    "def percentile_norm(image):\n",
    "    \"\"\"Normalize the image to the 99th percentile.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    for t, frame in tqdm(enumerate(image), desc=\"Normalizing...\"):\n",
    "        p99 = np.percentile(frame, 99)\n",
    "        p1 = np.percentile(frame, 1)\n",
    "        frame = (frame - p1) / (p99 - p1)\n",
    "        frame = (frame - np.min(frame)) / (np.max(frame) - np.min(frame))\n",
    "        # image[t] = np.clip(frame, 0, 1)\n",
    "        image[t] = frame\n",
    "        # print(f\"Min {np.min(image[t])}, Max {np.max(image[t])}, Mean {np.mean(image[t])}\")\n",
    "        # print(f\"Min {np.min(frame)}, Max {np.max(frame)}, Mean {np.mean(frame)}\")\n",
    "    return image\n",
    "\n",
    "example_image = percentile_norm(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e880e84-56e8-4266-a972-6b57dc961dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25.3M/25.3M [00:02<00:00, 12.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "cellpose_model = models.CellposeModel(gpu=True, model_type='cyto3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68986522-b18e-42c8-8c0e-c92c60b9ed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [01:18<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "masks = np.zeros_like(example_image, dtype=np.uint16)\n",
    "for i in tqdm(range(example_image.shape[0])):\n",
    "    masks[i], flows, styles = cellpose_model.eval(\n",
    "        example_image[i], diameter=45, do_3D=False, channels=[0, 0], normalize=True, flow_threshold=0.6, cellprob_threshold=-1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e5674b-f74b-4b52-bafc-33494a0c052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df of centroids per frame for each label\n",
    "\n",
    "def get_centroids(masks):\n",
    "    \"\"\"Get centroids of each label in the masks.\"\"\"\n",
    "    centroids = []\n",
    "    for t, frame in enumerate(masks):\n",
    "        props = regionprops(frame)\n",
    "        for prop in props:\n",
    "            centroids.append({\n",
    "                \"t\": t,\n",
    "                \"label\": prop.label,\n",
    "                \"centroid\": prop.centroid\n",
    "            })\n",
    "    return pd.DataFrame(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca502abf-d47e-497b-8d9a-b7f906396476",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_df = get_centroids(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c95935-3dbe-419e-9f7d-902eccfd0fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 706, 706)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2860150-3854-4c8b-b7cb-d56237026348",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original image (frame 0)\n",
    "axes[0].imshow(example_image[0], cmap='gray')\n",
    "axes[0].set_title(\"Original Image (Frame 0)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display the mask (frame 0)\n",
    "axes[1].imshow(masks[0], cmap='jet', alpha=0.7)\n",
    "axes[1].set_title(\"Mask (Frame 0)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48b3f6-59d1-4314-baa2-8b37f2b68e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Prompt:\n",
    "    original_label: int\n",
    "    centroid: np.ndarray\n",
    "    positive_prompts_coords: np.ndarray\n",
    "    negative_prompts_coords: np.ndarray\n",
    "\n",
    "    def get_in_sam_format(self):\n",
    "        points = np.zeros((self.positive_prompts_coords.shape[0] + self.negative_prompts_coords.shape[0], 2), dtype=np.float32)\n",
    "        points[:self.positive_prompts_coords.shape[0]] = self.positive_prompts_coords\n",
    "        if self.negative_prompts_coords.shape[0] > 0:\n",
    "            points[self.positive_prompts_coords.shape[0]:] = self.negative_prompts_coords\n",
    "\n",
    "        prompt_type = np.zeros((self.positive_prompts_coords.shape[0] + self.negative_prompts_coords.shape[0],), dtype=np.int32)\n",
    "        prompt_type[:self.positive_prompts_coords.shape[0]] = 1  # Positive prompts\n",
    "        if self.negative_prompts_coords.shape[0] > 0:\n",
    "            prompt_type[self.positive_prompts_coords.shape[0]:] = 0  # Negative prompts\n",
    "        return points, prompt_type\n",
    "\n",
    "\n",
    "def sample_positive_and_negative_prompts(masks, centroids_df, n_samples_pos=10, n_samples_neg=10, vicinity=10, min_distance=5):\n",
    "    \"\"\"Samples positive points strictly inside the mask and negative points outside the mask within a vicinity.\"\"\"\n",
    "    n_labels = len(np.unique(masks[0])) - 1  # Ignore background label 0\n",
    "    prompts = []  # List to store Prompt objects for each label\n",
    "\n",
    "    for label in np.unique(masks[0])[1:]:  # Skip background label 0\n",
    "        # Get the centroid for the current label\n",
    "        label_centroids = centroids_df[(centroids_df[\"label\"] == label) & (centroids_df[\"t\"] == 0)][\"centroid\"].values\n",
    "        assert len(label_centroids) == 1, f\"Label {label} has {len(label_centroids)} centroids, instead of being unique.\"\n",
    "        centroid = label_centroids[0]\n",
    "\n",
    "        # Get the mask for the current label\n",
    "        mask = masks[0] == label\n",
    "\n",
    "        # Get all coordinates strictly within the mask (positive points)\n",
    "        mask_coords = np.column_stack(np.where(mask))\n",
    "\n",
    "        # Check if there are enough points to sample\n",
    "        if len(mask_coords) < n_samples_pos:\n",
    "            raise ValueError(f\"Not enough points in mask for label {label} to sample {n_samples_pos} points.\")\n",
    "\n",
    "        # Randomly sample positive points\n",
    "        sampled_pos_indices = np.random.choice(len(mask_coords), size=n_samples_pos, replace=False)\n",
    "        positive_prompts_coords = mask_coords[sampled_pos_indices]\n",
    "\n",
    "        # Define the vicinity region (expand the bounding box by `vicinity` pixels)\n",
    "        y_min, x_min = mask_coords.min(axis=0)\n",
    "        y_max, x_max = mask_coords.max(axis=0)\n",
    "        y_min_vicinity = max(0, y_min - vicinity)\n",
    "        x_min_vicinity = max(0, x_min - vicinity)\n",
    "        y_max_vicinity = min(masks[0].shape[0], y_max + vicinity)\n",
    "        x_max_vicinity = min(masks[0].shape[1], x_max + vicinity)\n",
    "\n",
    "        # Get all coordinates within the vicinity region\n",
    "        vicinity_mask = np.zeros_like(mask, dtype=bool)\n",
    "        vicinity_mask[y_min_vicinity:y_max_vicinity + 1, x_min_vicinity:x_max_vicinity + 1] = True\n",
    "\n",
    "        # Ensure negative points are strictly outside the mask\n",
    "        negative_mask = vicinity_mask & ~mask\n",
    "\n",
    "        # Remove points that are too close to the mask border\n",
    "        mask_border = np.zeros_like(mask, dtype=bool)\n",
    "        mask_border[max(0, y_min - min_distance):min(mask.shape[0], y_max + min_distance + 1),\n",
    "                    max(0, x_min - min_distance):min(mask.shape[1], x_max + min_distance + 1)] = True\n",
    "        negative_mask = negative_mask & ~mask_border\n",
    "\n",
    "        # Get all valid negative coordinates\n",
    "        outside_coords = np.column_stack(np.where(negative_mask))\n",
    "\n",
    "        # Check if there are enough points to sample\n",
    "        if len(outside_coords) < n_samples_neg:\n",
    "            raise ValueError(f\"Not enough points outside mask for label {label} to sample {n_samples_neg} points.\")\n",
    "\n",
    "        # Randomly sample negative points\n",
    "        sampled_neg_indices = np.random.choice(len(outside_coords), size=n_samples_neg, replace=False)\n",
    "        negative_prompts_coords = outside_coords[sampled_neg_indices]\n",
    "\n",
    "        # Create a Prompt object for the current label\n",
    "        prompt = Prompt(\n",
    "            original_label=label,\n",
    "            centroid=centroid,\n",
    "            positive_prompts_coords=positive_prompts_coords,\n",
    "            negative_prompts_coords=negative_prompts_coords\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d50db0-6663-4632-a85b-335d4e705fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not enough points in mask for label 1 to sample 4 points.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[43msample_positive_and_negative_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvicinity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 43\u001b[0m, in \u001b[0;36msample_positive_and_negative_prompts\u001b[0;34m(masks, centroids_df, n_samples_pos, n_samples_neg, vicinity, min_distance)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Check if there are enough points to sample\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mask_coords) \u001b[38;5;241m<\u001b[39m n_samples_pos:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough points in mask for label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m points.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Randomly sample positive points\u001b[39;00m\n\u001b[1;32m     46\u001b[0m sampled_pos_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(mask_coords), size\u001b[38;5;241m=\u001b[39mn_samples_pos, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough points in mask for label 1 to sample 4 points."
     ]
    }
   ],
   "source": [
    "prompts = sample_positive_and_negative_prompts(masks[0], centroids_df, n_samples_pos=4, n_samples_neg=3, vicinity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35aa20-9b30-4706-85d7-c824c4e4794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SAM2VideoPredictorWrapper(SAM2VideoPredictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_id, **kwargs) -> 'SAM2VideoPredictorWrapper':\n",
    "        \"\"\"Load a pretrained model.\"\"\"\n",
    "        from sam2.build_sam import build_sam2_video_predictor_hf\n",
    "\n",
    "        sam_model = build_sam2_video_predictor_hf(model_id, **kwargs)\n",
    "        wrapper_instance = cls(\n",
    "            image_encoder=sam_model.image_encoder,\n",
    "            memory_attention=sam_model.memory_attention, \n",
    "            memory_encoder=sam_model.memory_encoder\n",
    "        )\n",
    "        wrapper_instance.__dict__.update(sam_model.__dict__)  # Copy all attributes from the base model\n",
    "        wrapper_instance.to(wrapper_instance.device)  # Move to the correct device\n",
    "        return wrapper_instance\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def init_state(\n",
    "        self,\n",
    "        # video_path,\n",
    "        video_array,\n",
    "        offload_video_to_cpu=False,\n",
    "        offload_state_to_cpu=False,\n",
    "        # async_loading_frames=False,\n",
    "    ):\n",
    "        \"\"\"Initialize an inference state.\"\"\"\n",
    "        compute_device = self.device  # device of the model\n",
    "        # images, video_height, video_width = load_video_frames(\n",
    "        #     video_path=video_path,\n",
    "        #     image_size=self.image_size,\n",
    "        #     offload_video_to_cpu=offload_video_to_cpu,\n",
    "        #     async_loading_frames=async_loading_frames,\n",
    "        #     compute_device=compute_device,\n",
    "        # )\n",
    "        if len(video_array.shape) == 3:\n",
    "            # add fake RGB channels by repeating the image 3 times at axis 1\n",
    "            video_array = np.repeat(video_array[:, :, :, np.newaxis], 3, axis=3)\n",
    "        video_array = video_array.swapaxes(3, 1)\n",
    "        print(f\"Input video shape: {video_array.shape}\")\n",
    "        video_height, video_width = video_array.shape[-2:]\n",
    "        images = torch.from_numpy(video_array).to(compute_device)\n",
    "        images = F.interpolate(images, size=(self.image_size, self.image_size), mode=\"bilinear\", align_corners=False)\n",
    "        print(f\"Input image shape: {images.shape}\")\n",
    "        inference_state = {}\n",
    "        inference_state[\"images\"] = images\n",
    "        inference_state[\"num_frames\"] = len(images)\n",
    "        # whether to offload the video frames to CPU memory\n",
    "        # turning on this option saves the GPU memory with only a very small overhead\n",
    "        inference_state[\"offload_video_to_cpu\"] = offload_video_to_cpu\n",
    "        # whether to offload the inference state to CPU memory\n",
    "        # turning on this option saves the GPU memory at the cost of a lower tracking fps\n",
    "        # (e.g. in a test case of 768x768 model, fps dropped from 27 to 24 when tracking one object\n",
    "        # and from 24 to 21 when tracking two objects)\n",
    "        inference_state[\"offload_state_to_cpu\"] = offload_state_to_cpu\n",
    "        # the original video height and width, used for resizing final output scores\n",
    "        inference_state[\"video_height\"] = video_height\n",
    "        inference_state[\"video_width\"] = video_width\n",
    "        inference_state[\"device\"] = compute_device\n",
    "        if offload_state_to_cpu:\n",
    "            inference_state[\"storage_device\"] = torch.device(\"cpu\")\n",
    "        else:\n",
    "            inference_state[\"storage_device\"] = compute_device\n",
    "        # inputs on each frame\n",
    "        inference_state[\"point_inputs_per_obj\"] = {}\n",
    "        inference_state[\"mask_inputs_per_obj\"] = {}\n",
    "        # visual features on a small number of recently visited frames for quick interactions\n",
    "        inference_state[\"cached_features\"] = {}\n",
    "        # values that don't change across frames (so we only need to hold one copy of them)\n",
    "        inference_state[\"constants\"] = {}\n",
    "        # mapping between client-side object id and model-side object index\n",
    "        inference_state[\"obj_id_to_idx\"] = OrderedDict()\n",
    "        inference_state[\"obj_idx_to_id\"] = OrderedDict()\n",
    "        inference_state[\"obj_ids\"] = []\n",
    "        # Slice (view) of each object tracking results, sharing the same memory with \"output_dict\"\n",
    "        inference_state[\"output_dict_per_obj\"] = {}\n",
    "        # A temporary storage to hold new outputs when user interact with a frame\n",
    "        # to add clicks or mask (it's merged into \"output_dict\" before propagation starts)\n",
    "        inference_state[\"temp_output_dict_per_obj\"] = {}\n",
    "        # Frames that already holds consolidated outputs from click or mask inputs\n",
    "        # (we directly use their consolidated outputs during tracking)\n",
    "        # metadata for each tracking frame (e.g. which direction it's tracked)\n",
    "        inference_state[\"frames_tracked_per_obj\"] = {}\n",
    "        # Warm up the visual backbone and cache the image feature on frame 0\n",
    "        self._get_image_feature(inference_state, frame_idx=0, batch_size=1)\n",
    "        return inference_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01c3fe-7b1f-4e59-822b-57d4da93438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SAM2VideoPredictorWrapper.from_pretrained(\"facebook/sam2.1-hiera-base-plus\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c075fe1-f79b-40fd-8d16-f9f79a87972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410a755-9ded-4b0c-8716-18490075097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prompts:\n",
    "    tracker_id = p.original_label\n",
    "    points, labels = p.get_in_sam_format()\n",
    "    print(f\"Adding {points.shape[0]} points for tracker {tracker_id}.\")\n",
    "    _, object_ids, mask_logits = predictor.add_new_points(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=0,\n",
    "        obj_id=tracker_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2bb62-748e-481d-99a1-51ff9a4d4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd43c67-e73b-4deb-b905-34b435a2dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_propagated = np.zeros_like(masks, dtype=np.uint8)\n",
    "for t, frame in enumerate(masks_propagated):\n",
    "    for obj_id, mask in video_segments[t].items():\n",
    "        mask = mask.squeeze().swapaxes(0, 1)\n",
    "        masks_propagated[t][mask] = obj_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `example_image` is the original image and `masks_propagated` is the propagated masks\n",
    "time_steps = masks_propagated.shape[0]\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for the original image and the propagated masks for each time step\n",
    "for t in range(time_steps):\n",
    "    # Add the original image\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=example_image[t],\n",
    "        colorscale='Gray',\n",
    "        showscale=False,\n",
    "        visible=(t == 0),  # Only the first frame is visible initially\n",
    "        name=f\"Original Image {t}\"\n",
    "    ))\n",
    "    # Add the propagated masks\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=masks_propagated[t],\n",
    "        colorscale='Jet',\n",
    "        showscale=False,\n",
    "        visible=(t == 0),  # Only the first frame is visible initially\n",
    "        name=f\"Propagated Mask {t}\"\n",
    "    ))\n",
    "\n",
    "# Update layout for the slider\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        {\n",
    "            \"buttons\": [\n",
    "                {\n",
    "                    \"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": True}, \"fromcurrent\": True}],\n",
    "                    \"label\": \"Play\",\n",
    "                    \"method\": \"animate\"\n",
    "                },\n",
    "                {\n",
    "                    \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": True}, \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
    "                    \"label\": \"Pause\",\n",
    "                    \"method\": \"animate\"\n",
    "                }\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"pad\": {\"r\": 10, \"t\": 87},\n",
    "            \"showactive\": False,\n",
    "            \"type\": \"buttons\",\n",
    "            \"x\": 0.1,\n",
    "            \"xanchor\": \"right\",\n",
    "            \"y\": 0,\n",
    "            \"yanchor\": \"top\"\n",
    "        }\n",
    "    ],\n",
    "    sliders=[\n",
    "        {\n",
    "            \"active\": 0,\n",
    "            \"yanchor\": \"top\",\n",
    "            \"xanchor\": \"left\",\n",
    "            \"currentvalue\": {\n",
    "                \"font\": {\"size\": 20},\n",
    "                \"prefix\": \"Time: \",\n",
    "                \"visible\": True,\n",
    "                \"xanchor\": \"right\"\n",
    "            },\n",
    "            \"transition\": {\"duration\": 300, \"easing\": \"cubic-in-out\"},\n",
    "            \"pad\": {\"b\": 10, \"t\": 50},\n",
    "            \"len\": 0.9,\n",
    "            \"x\": 0.1,\n",
    "            \"y\": 0,\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"args\": [[f\"frame{t}\"], {\"frame\": {\"duration\": 300, \"redraw\": True}, \"mode\": \"immediate\", \"transition\": {\"duration\": 300}}],\n",
    "                    \"label\": str(t),\n",
    "                    \"method\": \"animate\"\n",
    "                } for t in range(time_steps)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add frames for animation\n",
    "frames = []\n",
    "for t in range(time_steps):\n",
    "    frames.append(go.Frame(\n",
    "        data=[\n",
    "            go.Heatmap(z=example_image[t], colorscale='Gray', showscale=False),\n",
    "            go.Heatmap(z=masks_propagated[t], colorscale='Jet', showscale=False)\n",
    "        ],\n",
    "        name=f\"frame{t}\"\n",
    "    ))\n",
    "fig.frames = frames\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
